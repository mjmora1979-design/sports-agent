"""
monte_carlo_model.py
--------------------
Runs Monte Carlo simulations using the model-adjusted probabilities
generated by model_payload.build_model_payload().

Each matchup is simulated N times to estimate expected value (EV)
and recommended Kelly-lite stake size.
Now includes fuzzy calibration for comparing model accuracy vs. results.
"""

import numpy as np
import pandas as pd
from datetime import datetime
import os, json
from fuzzywuzzy import process
from model_payload import build_model_payload
from sports_agent import build_payload


# ------------------------------------------------------------
# Core Monte Carlo simulation
# ------------------------------------------------------------
def simulate_matchup(home_prob: float, away_prob: float, n_sims: int = 20000):
    """Simulate N games using adjusted win probabilities."""
    if np.isnan(home_prob) or np.isnan(away_prob):
        return np.nan, np.nan, np.nan

    draws = np.random.rand(n_sims)
    home_wins = np.sum(draws < home_prob)
    away_wins = n_sims - home_wins

    home_win_pct = home_wins / n_sims
    away_win_pct = away_wins / n_sims
    std_error = np.sqrt(home_prob * (1 - home_prob) / n_sims)
    return home_win_pct, away_win_pct, std_error


def kelly_fraction(edge: float, odds: float, fraction_cap: float = 0.25):
    """Compute 'Kelly-lite' staking fraction based on EV edge."""
    try:
        b = abs(odds) / 100 if odds < 0 else odds / 100
        q = 1 - (1 / (b + 1))
        kelly = ((b * (edge / 100)) - q) / b
        return max(0, min(kelly, fraction_cap))
    except Exception:
        return 0.0


# ------------------------------------------------------------
# Simulation runner
# ------------------------------------------------------------
def run_monte_carlo(snapshot_type="opening", n_sims=20000, sim_confidence=0.8):
    """
    Builds model payload, runs Monte Carlo simulations,
    and returns both matchup-level (df) and team-level (plays_df) datasets.
    """
    print(f"[INFO] Running Monte Carlo: {snapshot_type} ({n_sims:,} sims per matchup)")

    # Build data payload
    raw_json = build_payload("nfl", snapshot_type)
    model_df = build_model_payload(raw_json, snapshot_type=snapshot_type, sim_confidence=sim_confidence)

    results = []
    for _, row in model_df.iterrows():
        home_prob = row["home_fair_prob"]
        away_prob = row["away_fair_prob"]

        home_win_pct, away_win_pct, std_err = simulate_matchup(home_prob, away_prob, n_sims)

        home_ev = (home_win_pct - row["home_ml_prob"]) * 100
        away_ev = (away_win_pct - row["away_ml_prob"]) * 100

        home_kelly = kelly_fraction(home_ev, row["home_ml"] if pd.notna(row["home_ml"]) else -110)
        away_kelly = kelly_fraction(away_ev, row["away_ml"] if pd.notna(row["away_ml"]) else -110)

        results.append({
            "bookmaker": row["bookmaker"],
            "home_team": row["home_team"],
            "away_team": row["away_team"],
            "home_ml": row["home_ml"],
            "away_ml": row["away_ml"],
            "home_prob_model": round(home_prob, 4),
            "home_win_sim": round(home_win_pct, 4),
            "home_EV_%": round(home_ev, 2),
            "home_Kelly_frac": round(home_kelly, 3),
            "away_prob_model": round(away_prob, 4),
            "away_win_sim": round(away_win_pct, 4),
            "away_EV_%": round(away_ev, 2),
            "away_Kelly_frac": round(away_kelly, 3),
            "std_error": round(std_err, 5),
            "snapshot_type": snapshot_type,
            "generated_at": datetime.utcnow().isoformat()
        })

    df = pd.DataFrame(results)

    # Team-level view for both sides
    all_plays = []
    for _, r in df.iterrows():
        all_plays.append({
            "bookmaker": r["bookmaker"],
            "team_side": "Home",
            "team": r["home_team"],
            "opponent": r["away_team"],
            "odds": r["home_ml"],
            "EV_%": r["home_EV_%"],
            "Kelly_frac": r["home_Kelly_frac"]
        })
        all_plays.append({
            "bookmaker": r["bookmaker"],
            "team_side": "Away",
            "team": r["away_team"],
            "opponent": r["home_team"],
            "odds": r["away_ml"],
            "EV_%": r["away_EV_%"],
            "Kelly_frac": r["away_Kelly_frac"]
        })

    plays_df = pd.DataFrame(all_plays)

    # Show top 5 overall EV plays
    top5 = plays_df.sort_values(by="EV_%", ascending=False).head(5)
    print("\n🏈 Top 5 Overall Value Opportunities (by EV %)")
    print(top5[["bookmaker", "team_side", "team", "opponent", "odds", "EV_%", "Kelly_frac"]])

    # Save results
    df.to_csv("sim_output_full.csv", index=False)
    plays_df.to_csv("value_opportunities.csv", index=False)
    print("💾 Saved outputs → sim_output_full.csv and value_opportunities.csv")

    return df, plays_df


# ------------------------------------------------------------
# Calibration tracker (with fuzzy matching)
# ------------------------------------------------------------
def calibrate_model(sim_df: pd.DataFrame, results_path="final_scores.csv"):
    """Compare simulated win% vs actual results, using fuzzy matching for teams."""
    if not os.path.exists(results_path):
        print(f"[WARN] No results file found at {results_path}. Skipping calibration.")
        return None

    actuals = pd.read_csv(results_path)

    def normalize_team(name):
        return str(name).lower().replace(" ", "").replace(".", "").replace("-", "")

    sim_df["home_norm"] = sim_df["home_team"].apply(normalize_team)
    sim_df["away_norm"] = sim_df["away_team"].apply(normalize_team)
    actuals["home_norm"] = actuals["home_team"].apply(normalize_team)
    actuals["away_norm"] = actuals["away_team"].apply(normalize_team)

    merged_rows = []
    unmatched = []

    for _, sim in sim_df.iterrows():
        best_home = process.extractOne(sim["home_norm"], actuals["home_norm"])
        best_away = process.extractOne(sim["away_norm"], actuals["away_norm"])
        if best_home and best_away and best_home[1] > 85 and best_away[1] > 85:
            match_row = actuals.loc[
                (actuals["home_norm"] == best_home[0]) &
                (actuals["away_norm"] == best_away[0])
            ]
            if not match_row.empty:
                merged_rows.append({**sim.to_dict(), **match_row.iloc[0].to_dict()})
        else:
            unmatched.append((sim["home_team"], sim["away_team"]))

    if not merged_rows:
        print("[WARN] No overlapping matchups even after fuzzy matching.")
        if unmatched:
            pd.DataFrame(unmatched, columns=["home_team", "away_team"]).to_csv("unmatched_games.csv", index=False)
            print("🔍 Saved unmatched games → unmatched_games.csv for review.")
        return None

    merged = pd.DataFrame(merged_rows)
    merged["predicted_winner"] = np.where(
        merged["home_win_sim"] > merged["away_win_sim"],
        merged["home_team"],
        merged["away_team"]
    )
    merged["correct"] = merged["predicted_winner"] == merged["winner"]

    accuracy = merged["correct"].mean() * 100
    print(f"\n📈 Model Calibration Accuracy: {accuracy:.2f}% on {len(merged)} games")

    merged["evaluated_at"] = datetime.utcnow().isoformat()
    merged.to_csv("calibration_log.csv", index=False)
    print("✅ Calibration log saved → calibration_log.csv")

    return merged


# ------------------------------------------------------------
# Local test / integration output
# ------------------------------------------------------------
if __name__ == "__main__":
    df, plays_df = run_monte_carlo(snapshot_type="opening", n_sims=20000, sim_confidence=0.8)
    print("\n✅ Monte Carlo run complete — sample output:")
    print(df.head())

    calibrate_model(df)

    output_json = {
        "timestamp": datetime.utcnow().isoformat(),
        "snapshot_type": "opening",
        "top_opportunities": plays_df.sort_values(by="EV_%", ascending=False).head(10).to_dict(orient="records")
    }

    with open("monte_carlo_output.json", "w") as f:
        json.dump(output_json, f, indent=2)

    print("📤 JSON export saved → monte_carlo_output.json")
