"""
monte_carlo_model.py
--------------------
Runs Monte Carlo simulations using the model-adjusted probabilities
generated by model_payload.build_model_payload().

Each matchup is simulated N times to estimate expected value (EV)
and recommended Kelly-lite stake size.
Now includes a calibration tracker for comparing model accuracy vs. results.
"""

import numpy as np
import pandas as pd
from datetime import datetime
import os
from model_payload import build_model_payload
from sports_agent import build_payload


# ------------------------------------------------------------
# Core Monte Carlo simulation
# ------------------------------------------------------------
def simulate_matchup(home_prob: float, away_prob: float, n_sims: int = 20000):
    """
    Simulate N games using the adjusted win probabilities.
    Returns simulated win %, variance, and confidence interval.
    """
    if np.isnan(home_prob) or np.isnan(away_prob):
        return np.nan, np.nan, np.nan

    draws = np.random.rand(n_sims)
    home_wins = np.sum(draws < home_prob)
    away_wins = n_sims - home_wins

    home_win_pct = home_wins / n_sims
    away_win_pct = away_wins / n_sims
    std_error = np.sqrt(home_prob * (1 - home_prob) / n_sims)

    return home_win_pct, away_win_pct, std_error


def kelly_fraction(edge: float, odds: float, fraction_cap: float = 0.25):
    """
    Compute a 'Kelly-lite' staking fraction based on EV edge.
    Caps stake to avoid over-exposure.
    """
    try:
        b = abs(odds) / 100 if odds < 0 else odds / 100
        q = 1 - (1 / (b + 1))
        kelly = ((b * (edge / 100)) - q) / b
        return max(0, min(kelly, fraction_cap))
    except Exception:
        return 0.0


# ------------------------------------------------------------
# Simulation runner
# ------------------------------------------------------------
def run_monte_carlo(snapshot_type="opening", n_sims=20000, sim_confidence=0.8):
    """
    Builds model payload, runs Monte Carlo simulations, and returns DataFrame
    with simulated win %, EV %, and Kelly stake recommendation.
    """
    print(f"[INFO] Running Monte Carlo: {snapshot_type} ({n_sims:,} sims per matchup)")

    # Get odds + model probabilities
    raw_json = build_payload("nfl", snapshot_type)
    model_df = build_model_payload(raw_json, snapshot_type=snapshot_type, sim_confidence=sim_confidence)

    results = []
    for _, row in model_df.iterrows():
        home_prob = row["home_fair_prob"]
        away_prob = row["away_fair_prob"]

        home_win_pct, away_win_pct, std_err = simulate_matchup(home_prob, away_prob, n_sims)

        # Expected Value % based on difference between simulated win% and market probability
        home_ev = (home_win_pct - row["home_ml_prob"]) * 100
        away_ev = (away_win_pct - row["away_ml_prob"]) * 100

        # Kelly-lite staking
        home_kelly = kelly_fraction(home_ev, row["home_ml"] if pd.notna(row["home_ml"]) else -110)
        away_kelly = kelly_fraction(away_ev, row["away_ml"] if pd.notna(row["away_ml"]) else -110)

        results.append({
            "bookmaker": row["bookmaker"],
            "home_team": row["home_team"],
            "away_team": row["away_team"],
            "home_ml": row["home_ml"],
            "away_ml": row["away_ml"],
            "home_prob_model": round(home_prob, 4),
            "home_win_sim": round(home_win_pct, 4),
            "home_EV_%": round(home_ev, 2),
            "home_Kelly_frac": round(home_kelly, 3),
            "away_prob_model": round(away_prob, 4),
            "away_win_sim": round(away_win_pct, 4),
            "away_EV_%": round(away_ev, 2),
            "away_Kelly_frac": round(away_kelly, 3),
            "std_error": round(std_err, 5),
            "snapshot_type": snapshot_type,
            "generated_at": datetime.utcnow().isoformat()
        })

    df = pd.DataFrame(results)

    # âœ… Deduplicate by matchup for display
    df_unique = (
        df.sort_values(by="home_EV_%", ascending=False)
          .drop_duplicates(subset=["home_team", "away_team"], keep="first")
    )

    df_sorted = df_unique.head(5)

    print("\nðŸˆ Top 5 Unique Home-side opportunities (by EV %)")
    print(df_sorted[["bookmaker", "home_team", "away_team", "home_ml", "home_EV_%", "home_Kelly_frac"]].to_string(index=False))

    return df


# ------------------------------------------------------------
# Calibration tracker
# ------------------------------------------------------------
def calibrate_model(sim_df: pd.DataFrame, results_path="final_scores.csv"):
    """
    Compares simulated win% vs actual results (from final_scores.csv)
    and outputs calibration stats.
    The CSV should have columns: home_team, away_team, winner
    """
    if not os.path.exists(results_path):
        print(f"[WARN] No results file found at {results_path}. Skipping calibration.")
        return None

    actuals = pd.read_csv(results_path)
    merged = pd.merge(sim_df, actuals, on=["home_team", "away_team"], how="inner")

    # Assign predicted winner based on higher simulated win%
    merged["predicted_winner"] = np.where(
        merged["home_win_sim"] > merged["away_win_sim"],
        merged["home_team"],
        merged["away_team"]
    )

    merged["correct"] = merged["predicted_winner"] == merged["winner"]

    accuracy = merged["correct"].mean() * 100
    print(f"\nðŸ“ˆ Model Calibration Accuracy: {accuracy:.2f}% on {len(merged)} games")

    # Optional: Save a log file for tracking calibration over time
    merged["evaluated_at"] = datetime.utcnow().isoformat()
    merged.to_csv("calibration_log.csv", index=False)
    print("âœ… Calibration log saved â†’ calibration_log.csv")

    return merged


# ------------------------------------------------------------
# Local test
# ------------------------------------------------------------
if __name__ == "__main__":
    df = run_monte_carlo(snapshot_type="opening", n_sims=20000, sim_confidence=0.8)
    print("\nâœ… Monte Carlo run complete â€” sample output:")
    print(df.head())

    # Run calibration check if score file exists
    calibrate_model(df)
