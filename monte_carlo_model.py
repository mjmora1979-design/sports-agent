"""
monte_carlo_model.py
--------------------
Runs Monte Carlo simulations using the model-adjusted probabilities
generated by model_payload.build_model_payload().

Each matchup is simulated N times to estimate expected value (EV)
and recommended Kelly-lite stake size.
Now includes calibration persistence and optional parameter bias adjustments.
"""

import numpy as np
import pandas as pd
from datetime import datetime
import json, os
from typing import Optional
from model_payload import build_model_payload
from sports_agent import build_payload


# ------------------------------------------------------------
# Calibration persistence helpers
# ------------------------------------------------------------
def save_calibration(params: dict, filename: str = "calibrated_params.json"):
    """Save calibration parameters to disk."""
    with open(filename, "w") as f:
        json.dump(params, f, indent=2)
    print(f"✅ Calibration parameters saved → {filename}")


def load_calibration(filename: str = "calibrated_params.json") -> Optional[dict]:
    """Load calibration parameters if available."""
    if not os.path.exists(filename):
        print(f"[INFO] No calibration file found at {filename}, using defaults.")
        return None
    with open(filename, "r") as f:
        params = json.load(f)
    print(f"[INFO] Loaded calibration parameters from {filename}")
    return params


def apply_calibration(home_prob: float, away_prob: float, calib: Optional[dict]):
    """Apply calibration adjustments to raw probabilities."""
    if calib is None:
        return home_prob, away_prob

    # Example calibration adjustments
    home_adj = calib.get("home_bias_adjustment", 1.0)
    away_adj = calib.get("away_bias_adjustment", 1.0)

    home_prob *= home_adj
    away_prob *= away_adj

    # Renormalize to ensure probabilities sum to 1
    total = home_prob + away_prob
    if total > 0:
        home_prob /= total
        away_prob /= total

    return home_prob, away_prob


# ------------------------------------------------------------
# Core Monte Carlo simulation
# ------------------------------------------------------------
def simulate_matchup(home_team, away_team, home_prob, away_prob, n_sims=20000, calib: Optional[dict] = None):
    """
    Simulate N games using (optionally calibrated) win probabilities.
    Returns simulated win %, variance, and confidence interval.
    """
    home_prob, away_prob = apply_calibration(home_prob, away_prob, calib)

    draws = np.random.rand(n_sims)
    home_wins = np.sum(draws < home_prob)
    away_wins = n_sims - home_wins

    home_win_pct = home_wins / n_sims
    away_win_pct = away_wins / n_sims
    std_error = np.sqrt(home_prob * (1 - home_prob) / n_sims)

    return home_win_pct, away_win_pct, std_error


def kelly_fraction(edge: float, odds: float, fraction_cap: float = 0.25):
    """
    Compute a 'Kelly-lite' staking fraction based on EV edge.
    Caps stake to avoid over-exposure.
    """
    try:
        b = abs(odds) / 100 if odds < 0 else odds / 100
        q = 1 - (1 / (b + 1))
        kelly = ((b * (edge / 100)) - q) / b
        return max(0, min(kelly, fraction_cap))
    except Exception:
        return 0.0


# ------------------------------------------------------------
# Simulation runner
# ------------------------------------------------------------
def run_monte_carlo(snapshot_type="opening", n_sims=20000, sim_confidence=0.8):
    """
    Builds model payload, runs Monte Carlo simulations, and returns DataFrame
    with simulated win %, EV %, and Kelly stake recommendation.
    """
    print(f"[INFO] Running Monte Carlo: {snapshot_type} ({n_sims:,} sims per matchup)")

    # Load calibration if it exists
    calib = load_calibration()

    # Get odds + model probabilities
    raw_json = build_payload("nfl", snapshot_type)
    model_df = build_model_payload(raw_json, snapshot_type=snapshot_type, sim_confidence=sim_confidence)

    results = []
    for _, row in model_df.iterrows():
        home_prob = row["home_fair_prob"]
        away_prob = row["away_fair_prob"]

        home_win_pct, away_win_pct, std_err = simulate_matchup(
            row["home_team"], row["away_team"], home_prob, away_prob, n_sims, calib
        )

        # Expected Value % based on difference between simulated win% and market probability
        home_ev = (home_win_pct - row["home_ml_prob"]) * 100
        away_ev = (away_win_pct - row["away_ml_prob"]) * 100

        # Kelly-lite staking
        home_kelly = kelly_fraction(home_ev, row["home_ml"] if pd.notna(row["home_ml"]) else -110)
        away_kelly = kelly_fraction(away_ev, row["away_ml"] if pd.notna(row["away_ml"]) else -110)

        results.append({
            "bookmaker": row["bookmaker"],
            "home_team": row["home_team"],
            "away_team": row["away_team"],
            "home_ml": row["home_ml"],
            "away_ml": row["away_ml"],
            "home_prob_model": round(home_prob, 4),
            "home_win_sim": round(home_win_pct, 4),
            "home_EV_%": round(home_ev, 2),
            "home_Kelly_frac": round(home_kelly, 3),
            "away_prob_model": round(away_prob, 4),
            "away_win_sim": round(away_win_pct, 4),
            "away_EV_%": round(away_ev, 2),
            "away_Kelly_frac": round(away_kelly, 3),
            "std_error": round(std_err, 5),
            "snapshot_type": snapshot_type,
            "generated_at": datetime.utcnow().isoformat()
        })

    df = pd.DataFrame(results)

    df_unique = (
        df.sort_values(by="home_EV_%", ascending=False)
          .drop_duplicates(subset=["home_team", "away_team"], keep="first")
    )

    df_sorted = df_unique.head(5)

    print("\n🏈 Top 5 Unique Home-side opportunities (by EV %)")
    print(df_sorted[["bookmaker", "home_team", "away_team", "home_ml", "home_EV_%", "home_Kelly_frac"]].to_string(index=False))

    return df


# ------------------------------------------------------------
# Calibration tracker
# ------------------------------------------------------------
def calibrate_model(sim_df: pd.DataFrame, results_path="final_scores.csv"):
    """
    Compares simulated win% vs actual results and outputs calibration stats.
    Also returns suggested adjustment parameters.
    """
    if not os.path.exists(results_path):
        print(f"[WARN] No results file found at {results_path}. Skipping calibration.")
        return None

    actuals = pd.read_csv(results_path)
    merged = pd.merge(sim_df, actuals, on=["home_team", "away_team"], how="inner")

    if merged.empty:
        print("[WARN] No overlapping games found for calibration.")
        return None

    merged["predicted_winner"] = np.where(
        merged["home_win_sim"] > merged["away_win_sim"],
        merged["home_team"],
        merged["away_team"]
    )
    merged["correct"] = merged["predicted_winner"] == merged["winner"]

    accuracy = merged["correct"].mean() * 100
    print(f"\n📈 Model Calibration Accuracy: {accuracy:.2f}% on {len(merged)} games")

    # Example bias calibration (simple heuristic)
    home_bias_adj = 1.0 + ((merged["home_win_sim"].mean() - 0.5) * 0.1)
    away_bias_adj = 1.0 + ((merged["away_win_sim"].mean() - 0.5) * 0.1)

    calib = {
        "home_bias_adjustment": round(home_bias_adj, 3),
        "away_bias_adjustment": round(away_bias_adj, 3),
        "accuracy": round(accuracy, 2),
        "evaluated_at": datetime.utcnow().isoformat()
    }

    save_calibration(calib)
    merged["evaluated_at"] = datetime.utcnow().isoformat()
    merged.to_csv("calibration_log.csv", index=False)
    print("✅ Calibration log saved → calibration_log.csv")

    return calib


# ------------------------------------------------------------
# Local test
# ------------------------------------------------------------
if __name__ == "__main__":
    df = run_monte_carlo(snapshot_type="opening", n_sims=20000, sim_confidence=0.8)
    print("\n✅ Monte Carlo run complete — sample output:")
    print(df.head())

    # Run calibration check if score file exists
    calibrate_model(df)
